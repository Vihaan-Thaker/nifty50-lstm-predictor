{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0047d9c-3edb-49f2-b74a-cee86003be49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import yfinance as yf\n",
    "import tensorflow as tf\n",
    "from datetime import datetime , timedelta\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "from ta.momentum import RSIIndicator\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a75401-5e40-4193-94c3-50ea5737f46e",
   "metadata": {},
   "source": [
    "#### Firstly we get the stickers for NIFTY50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20b8d97-7715-4b93-8891-ea326a3a0398",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://en.wikipedia.org/wiki/NIFTY_50'\n",
    "table = pd.read_html(url)\n",
    "\n",
    "nifty50_df = table[1]  # Table 1 contains the NIFTY 50 companies\n",
    "nifty50_tickers = nifty50_df['Symbol'].tolist()\n",
    "\n",
    "nifty50_tickers = [ticker + '.NS' for ticker in nifty50_tickers]\n",
    "\n",
    "print(nifty50_tickers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3316c404-240d-4560-865c-91d32fca3da7",
   "metadata": {},
   "source": [
    "#### Importing all the libraries required to build the LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fddd87e-0200-4eef-872b-c89746249a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential #This is required to make the Neural Network layer by layer\n",
    "from tensorflow.keras.layers import * #Used to import LSTM layer , Dense ( fully connected layer ) , Dropout ( Regularization to prevent overfitting)\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint #Automatically saves the best model even if later epochs overfit or degrade\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "from tensorflow.keras.optimizers import Adam #Used to adjust the model weights during training\n",
    "from tensorflow.keras.models import load_model\n",
    "from keras_tuner import RandomSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c027db9-d358-4e69-b541-88dc63aed83a",
   "metadata": {},
   "source": [
    "#### Below Function is called inside the train_model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3729c07-b8bf-4ddf-8f05-eb6a852a4e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(30, 6)))  # 6 input features\n",
    "\n",
    "    model.add(LSTM(units=hp.Int('lstm_units', min_value=32, max_value=128, step=16)))\n",
    "    model.add(Dropout(rate=hp.Float('dropout', 0.0, 0.5, step=0.1)))\n",
    "    model.add(Dense(units=hp.Int('dense_units', 8, 64, step=8), activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    optimizer = hp.Choice('optimizer', ['adam', 'rmsprop'])\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95e6049-ff48-4191-861e-753a8ea6e856",
   "metadata": {},
   "source": [
    "#### The below function is to get the ticker data for the particular ticker symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3f5d2c-ed48-41f0-b366-bcebe86c6bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ticker_data(symbol , start_date , end_date):\n",
    "    buffer_start_date = start_date - timedelta(days=300)\n",
    "    ticker = yf.Ticker(symbol)\n",
    "    data = ticker.history(start = buffer_start_date , end = end_date)\n",
    "\n",
    "    nifty_ticker = yf.Ticker('^NSEI')\n",
    "    nifty_data = nifty_ticker.history(start = start_date , end = end_date)\n",
    "    \n",
    "    data['Return'] = data['Close'].pct_change()\n",
    "    data['MA_50'] = data['Close'].rolling(window=50).mean()\n",
    "    data['MA_200'] = data['Close'].rolling(window=200).mean()\n",
    "    data['RSI'] = RSIIndicator(close=data['Close'],window=14).rsi()\n",
    "    data['Volatility'] = data['Return'].rolling(window=14).std()\n",
    "    \n",
    "    data = data.loc[pd.to_datetime(start_date).tz_localize('UTC'):]\n",
    "    nifty_data['nifty_returns'] = nifty_data['Close'].pct_change()\n",
    "    data = data.join(nifty_data['nifty_returns'])\n",
    "    \n",
    "    data[['Volatility','MA_50','MA_200','RSI']] = scaler.fit_transform(data[['Volatility','MA_50','MA_200','RSI']])\n",
    "    return data[['Return','Volatility','MA_50','MA_200','RSI','nifty_returns']].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c265278-5d33-478e-9ee5-89b50387a624",
   "metadata": {},
   "source": [
    "#### The Below function is just to convert a dataframe to a numpy array so it can be manipulated more easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011eec7e-9274-4507-9409-73d4ab22a0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_to_np(X , y):\n",
    "    X_np = np.array([df.values for df in X])\n",
    "    y_np = np.array(y)\n",
    "    return X_np , y_np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19c9710-411e-4b22-a0e7-73970a32d304",
   "metadata": {},
   "source": [
    "#### The Below function is to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e51938f-5ff1-44ec-b4b3-03800dc1e1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(symbol, train_input, train_label, validation_input, validation_labels):\n",
    "    print(f'Searching for best hyper-params for {symbol} model')\n",
    "    tuner = RandomSearch(\n",
    "        build_model,\n",
    "        objective='val_accuracy',\n",
    "        max_trials=20,\n",
    "        executions_per_trial=1,\n",
    "        directory='lstm_tuning',\n",
    "        project_name=f'nifty_predict_{symbol}'  # make it per-symbol to avoid overwrite\n",
    "    )\n",
    "\n",
    "    # Run hyperparameter search\n",
    "    tuner.search(train_input, train_label,\n",
    "                 validation_data=(validation_input, validation_labels),\n",
    "                 epochs=10)\n",
    "\n",
    "    # Get best model\n",
    "    model1 = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "    print(f'Hyper-Params for best model of {symbol} have been found')\n",
    "    # Save best weights\n",
    "    cp = ModelCheckpoint(f'model1_{symbol}/model.keras', save_best_only=True)\n",
    "    model1.fit(train_input, train_label,\n",
    "               validation_data=(validation_input, validation_labels),\n",
    "               epochs=10, callbacks=[cp])\n",
    "\n",
    "    # Reload best version and evaluate\n",
    "    model1 = load_model(f'model1_{symbol}/model.keras')\n",
    "    val_predictions = model1.predict(validation_input).flatten()\n",
    "    predicted_labels = (val_predictions > 0.5).astype(int)\n",
    "    accuracy = np.mean(predicted_labels == validation_labels)\n",
    "\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e1664e-45a0-41c9-a987-67831b4840ff",
   "metadata": {},
   "source": [
    "#### Below we call all the required functions and store the accuracies for each model in the respective dictionary\n",
    "##### Note that I have excluded a few companies while going through the tickers as they were having a problem while executing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aee7c5a-6056-4aad-a18d-f9af8b4fa971",
   "metadata": {},
   "outputs": [],
   "source": [
    "today = datetime.today().date()\n",
    "base_date = today - timedelta(days=180)\n",
    "validation_base_date = today - timedelta(days=70)\n",
    "accuracy_dict = {}\n",
    "tickers = nifty50_tickers[:13] + nifty50_tickers[15:40] + nifty50_tickers[41:]\n",
    "\n",
    "for symbol in tickers:\n",
    "    print('Entered the Loop for',symbol)\n",
    "    \n",
    "    # Initialize training window\n",
    "    start_date = base_date\n",
    "    end_date = start_date + timedelta(days=51)\n",
    "\n",
    "    # Initialize validation window\n",
    "    validation_start_date = validation_base_date\n",
    "    validation_end_date = validation_start_date + timedelta(days=51)\n",
    "\n",
    "    # Create validation dataset\n",
    "    validation_input = []\n",
    "    validation_labels = []\n",
    "    while validation_end_date < today:\n",
    "        validation_iteration_data = get_ticker_data(symbol, validation_start_date, validation_end_date)\n",
    "\n",
    "        if len(validation_iteration_data) < 31:\n",
    "            validation_start_date += timedelta(days=1)\n",
    "            validation_end_date += timedelta(days=1)\n",
    "            continue\n",
    "\n",
    "        label = int(validation_iteration_data['Return'].iloc[-1] > 0)\n",
    "        validation_iteration_data = validation_iteration_data.iloc[:-1].tail(30)\n",
    "\n",
    "        if len(validation_iteration_data) < 30:\n",
    "            validation_start_date += timedelta(days=1)\n",
    "            validation_end_date += timedelta(days=1)\n",
    "            continue\n",
    "        \n",
    "        validation_input.append(validation_iteration_data)\n",
    "        validation_labels.append(label)\n",
    "\n",
    "        validation_start_date += timedelta(days=1)\n",
    "        validation_end_date += timedelta(days=1)\n",
    "\n",
    "    validation_input, validation_labels = dataframe_to_np(validation_input, validation_labels)\n",
    "\n",
    "    print('Validation Data Collected for', symbol)\n",
    "    \n",
    "    # Create training dataset\n",
    "    input_data = []\n",
    "    output_labels = []\n",
    "    while end_date < validation_base_date:\n",
    "        data = get_ticker_data(symbol, start_date, end_date)\n",
    "\n",
    "        if len(data) < 30:\n",
    "            start_date += timedelta(days=1)\n",
    "            end_date += timedelta(days=1)\n",
    "            continue\n",
    "\n",
    "        label = int(data['Return'].iloc[-1] > 0)\n",
    "        data = data.iloc[:-1].tail(30)\n",
    "\n",
    "        if len(data) < 30:\n",
    "            start_date += timedelta(days=1)\n",
    "            end_date += timedelta(days=1)\n",
    "            continue\n",
    "\n",
    "        input_data.append(data)\n",
    "        output_labels.append(label)\n",
    "\n",
    "        start_date += timedelta(days=1)\n",
    "        end_date += timedelta(days=1)\n",
    "\n",
    "    input_data, output_labels = dataframe_to_np(input_data, output_labels)\n",
    "    # Build and train model\n",
    "    \n",
    "    print('Training Data Collected for',symbol)\n",
    "    \n",
    "    accuracy = train_model(symbol ,input_data , output_labels , validation_input , validation_labels)\n",
    "    accuracy_dict[symbol] = accuracy\n",
    "    \n",
    "    print('Loop done for',symbol)\n",
    "\n",
    "print('Execution Done ----- XXXX -----')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73b5a5c-79db-4102-bf85-43b35f4a49bc",
   "metadata": {},
   "source": [
    "#### Below I sort the accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2e7f5a-8798-4c04-97f7-16d3e2327803",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_accuracies = sorted(accuracy_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for symbol, accuracy in sorted_accuracies:\n",
    "    print(f\"{symbol}: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8d9c32-3e3c-4a01-9d8c-76f6338f5ef2",
   "metadata": {},
   "source": [
    "#### Below I only choose those companies who have accuracy > 60%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ccf9d8-17ab-4bff-ab37-7421dd89b5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.6\n",
    "high_accuracy_symbols = {\n",
    "    symbol: acc for symbol, acc in accuracy_dict.items() if acc > threshold\n",
    "}\n",
    "\n",
    "print(high_accuracy_symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422f1505-561e-4195-bce8-63cf01dd746f",
   "metadata": {},
   "source": [
    "#### The below function is to get the return today to check if my predicton is correct or not\n",
    "##### Note , call this function only after the trading day is over otherwise will give return of yesterday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06026c9-c025-477c-a85f-cfd884bf67a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_today_return(symbol):\n",
    "    ticker = yf.Ticker(symbol)\n",
    "    today_data = ticker.history(period='2d').Close\n",
    "    today_return = (today_data[-1] / today_data[0]) - 1\n",
    "    \n",
    "    return today_return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cea7965-b5b3-4c72-8c47-c614ff47d069",
   "metadata": {},
   "source": [
    "#### Below i check the return and compare it to my prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c29eff-04fa-4329-b473-1bcc2faef2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "today = datetime.today().date()\n",
    "start_date = today - timedelta(days=50)\n",
    "end_date = today - timedelta(days=1)\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "total_return = 0\n",
    "for symbol in high_accuracy_symbols :\n",
    "    total+=1\n",
    "    data = get_ticker_data(symbol, start_date, end_date)\n",
    "\n",
    "    if len(data) < 30:\n",
    "            print(f'Not Sufficient data for {symbol}')\n",
    "            continue\n",
    "    \n",
    "    data = data.tail(30)\n",
    "    input_np = np.array([data.values])\n",
    "    \n",
    "    model = load_model(f'model1_{symbol}/model.keras')\n",
    "    \n",
    "    prediction = model.predict(input_np)[0][0]\n",
    "    actual = get_today_return(symbol)\n",
    "    if int(actual > 0) == int(prediction > 0.5):\n",
    "        print(f'Prediction correct for {symbol}')\n",
    "        correct+=1\n",
    "        \n",
    "    else:\n",
    "        print(f'Prediction Wrong for {symbol}')\n",
    "        \n",
    "    total_return+=actual \n",
    "print('Our Accuracy for today is',correct / total)\n",
    "print('Our Total correct predictions are',correct)\n",
    "print('Total Companies in our Portfolio are',total)\n",
    "print('Our Total Return for today is',total_return)\n",
    "print('Predictions Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfd22a8-ae54-4f0d-983f-87a99b2d7316",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
